* Introduction
  In order to start our spark and scala back end project. Some basic understand of spark is necessary. Hence, The first park of this repo will be my understanding about the spark system. And the second part of this repo will be an tutorial I made for the scala and spark. Then, From the third part on will be the true beginning of our back end project.
  -----
  | Section | Name                      | Content                                          |
  |---------+---------------------------+--------------------------------------------------|
  |       1 | [[What is spark? Why spark?]] | Basic description of the spark system            |
  |       2 | [[Tutorial]]                  | An runnable spark tutorial (Scala)               |
  |       3 | [[Project description]]       | Describe the project structure and API in detail |
  #+TBLFM: $1=@#-1
* What is spark? Why spark?
   The most of the map reduce knowledge mention in this report is generated from the original google paper. (MapReduce: Simplified Data Processing on Large Clusters) 
** How can caching improve the map reduce?
*** What is Map Reduce looks like?
    The whole map reduce structure contains a set of workers and a master. Master is a scheduler, the user will submit tasks to the master and the master will assign the mission to the slave nodes. 
*** How Map Reduce works?
    The user submit a tasks. And the master nodes will first analysis that tasks and partition it and schedule and assign those works to the map and reducer. Map worker will read the input data (here cache may happen), than the map worker will generate the intermediate key/value pair, and this value/pair set can be cache to a disk (here cache may also happen). Many map reduce procedure will include more than 1 map reduce procedure. Thus we may need to cache the output of the map reduce procedure. 
*** Improve the performance
    Compared with the computation time cost by CPU, read data from the disk and send data via the network will cost lots of the real time. By reading the data from the local disk, and cache the intermediate data for a later usage, we can improve the total performance of the MapReduce procedure.
** Where does cache happens?
*** Input Data
    Master will assign the map mission to the nodes contains the data or the nodes "close" to the data.
*** Intermediate 
    writing a single copy of the intermediate data to local disk.
** Algorithm
   The algorithm for the cache in the map reduce procedure can be different for different routines. 
*** SQL
    Naive Map Reduce is quite simple, thus we hope to build a system on the top of the map reduce system that can implement the basic SQL query command. And in order to implement such query command we may need to do some optimization to cache the map reduce intermediate results and the output flow. I find some interesting paper discussing the some of the implementations like the pig (which is on top of the hadoop), Hive. However, I can't find the specific discuss about how the optimization in side it works, it is kind of frustrating.
*** Machine Learning (Spark)
    We will usually encounter a large scale of the data when dealing with the machine learning problems. And most of the time we implement a machine learning technique on top of the big data, we will perform many iterations. The naive method of the Map Reduce to deal with this problems is quite inefficiency. In fact, when we need to iterate something, it makes tons of sense to cache some of the results that we may need to use in the future iterations. Thus, the spark systems come out, it will ask user to give some hint about what needs to be cached and it will make it best effort to cache the data user indicates and generate the map reduce plan.
* Tutorial
  I will follow a sequence spark scala tutorial.
** Intro1
   Since we don't use the spark-shell here, we need to first import the spark library in our scala script
*** spark library
    #+BEGIN_SRC scala
      object hw {
        def main(args: Array[String]) {
          println("Hello World!")
        }
      }
    #+END_SRC

    #+results:
* Project description
** Goals
*** Find some way to iterate our products
    - have everyone on the team use the product
    - build a minimal product
      - viable :: Get product used everyday, everyone. Find bug and needs.
   
   
